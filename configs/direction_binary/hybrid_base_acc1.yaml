dataset_dir: data_preprocessed/acc_1_dataset

# training
batch_size: 32
lr: 0.0003        # typically lower LR for Transformer blocks
epochs: 100
early_stop: 15
max_folds: null   # null means run all subjects

# model â€“ CwA-Transformer specifics
enc_kernel: 7      # conv kernel size per channel
latent_dim: 64     # encoder latent dimension (unused in current impl but kept for future)
n_heads: 4         # multi-head attention heads
depth: 4           # number of Transformer encoder layers
dropout: 0.1
enc_stride: 2      # temporal down-sampling factor

# augmentation (copied from CNN baseline)
shift_p: 0.9771999065686038
shift_min_frac: 0.005
shift_max_frac: 0.04
scale_p: 0.08568568458523856
scale_min: 0.9
scale_max: 1.1
noise_p: 0.3
noise_std: 0.022713472820544915

# mixup & masking
mixup_alpha: 0.40991426550989196

time_mask_p: 0.22274360262040488
time_mask_frac: 0.15
chan_mask_p: 0.181364322319745
chan_mask_ratio: 0.10 